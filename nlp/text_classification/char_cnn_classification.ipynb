{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 228541 entries, 0 to 228540\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count   Dtype \n",
      "---  ------   --------------   ----- \n",
      " 0   keyword  228541 non-null  object\n",
      " 1   label    228541 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "keyword_df = pd.read_csv('../data/text_classification/brand_category_classification/keyword.csv')\n",
    "keyword_df['keyword'] = keyword_df['keyword'].replace(['NaN', 'nan'], pd.NA)\n",
    "keyword_df['keyword'] = keyword_df['keyword'].astype('str')\n",
    "keyword_df['keyword'] = keyword_df['keyword'].str.strip()\n",
    "\n",
    "keyword_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 59\n"
     ]
    }
   ],
   "source": [
    "max_length = keyword_df['keyword'].apply(len).max()\n",
    "\n",
    "print(f'max length: {max_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, char_to_idx, max_seq_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        try:\n",
    "            char_indices = [self.char_to_idx[c] for c in text if c in self.char_to_idx]\n",
    "            char_indices += [0] * (self.max_seq_length - len(char_indices))\n",
    "        except:\n",
    "            print(f\"idx: {idx}\")\n",
    "            print(f\"text: {text}\")\n",
    "\n",
    "        return torch.tensor(char_indices), torch.tensor(label)\n",
    "\n",
    "class CharCNN(nn.Module):\n",
    "    def __init__(self, num_features, embed_dim, num_classes):\n",
    "        super(CharCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_features, embed_dim)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(embed_dim, 128, kernel_size=7, padding=0)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.conv2 = nn.Conv1d(embed_dim, 128, kernel_size=7, padding=0)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.conv3 = nn.Conv1d(embed_dim, 128, kernel_size=3, padding=0)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.conv4 = nn.Conv1d(embed_dim, 128, kernel_size=3, padding=0)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.fc1 = nn.Linear(512, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x1 = F.relu(self.bn1(self.conv1(x)))\n",
    "        x2 = F.relu(self.bn2(self.conv2(x)))\n",
    "        x3 = F.relu(self.bn3(self.conv3(x)))\n",
    "        x4 = F.relu(self.bn4(self.conv4(x)))\n",
    "        x1 = F.max_pool1d(x1, x1.size(2)).squeeze(2)\n",
    "        x2 = F.max_pool1d(x2, x2.size(2)).squeeze(2)\n",
    "        x3 = F.max_pool1d(x3, x3.size(2)).squeeze(2)\n",
    "        x4 = F.max_pool1d(x4, x4.size(2)).squeeze(2)\n",
    "        x = torch.cat((x1, x2, x3, x4), 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1903"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(keyword_df['label'].to_list())\n",
    "\n",
    "texts = keyword_df['keyword'].to_list()\n",
    "labels = label_encoder.transform(keyword_df['label'].to_list())\n",
    "\n",
    "charset = set(\"\".join(texts))\n",
    "num_features = len(charset)\n",
    "\n",
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx = {char: idx for idx, char in enumerate(charset)}\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "train_dataset = CustomDataset(train_texts, train_labels, char_to_idx, max_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(val_texts, val_labels, char_to_idx, max_length)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "test_dataset = CustomDataset(test_texts, test_labels, char_to_idx, max_length)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "num_classes = 2\n",
    "learning_rate = 0.01\n",
    "num_epochs = 30\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "# device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 2023-09-27 12:33:36.554880\n",
      "2023-09-27 12:36:08.256139 - epoch [1/30] loss: 0.0294 val acc: 96.51%\n",
      "2023-09-27 12:38:38.899432 - epoch [2/30] loss: 0.0167 val acc: 96.93%\n",
      "2023-09-27 12:41:10.864629 - epoch [3/30] loss: 0.0045 val acc: 96.95%\n",
      "2023-09-27 12:43:38.315852 - epoch [4/30] loss: 0.0267 val acc: 97.13%\n",
      "2023-09-27 12:46:06.181445 - epoch [5/30] loss: 0.0135 val acc: 97.29%\n",
      "2023-09-27 12:48:31.989002 - epoch [6/30] loss: 0.0199 val acc: 97.20%\n",
      "2023-09-27 12:50:56.623061 - epoch [7/30] loss: 0.0247 val acc: 97.56%\n",
      "2023-09-27 12:53:24.849731 - epoch [8/30] loss: 0.0253 val acc: 97.55%\n",
      "2023-09-27 12:55:46.389853 - epoch [9/30] loss: 0.0160 val acc: 97.49%\n",
      "2023-09-27 12:58:00.652078 - epoch [10/30] loss: 0.0084 val acc: 97.60%\n",
      "2023-09-27 13:00:14.891079 - epoch [11/30] loss: 0.0100 val acc: 97.67%\n",
      "2023-09-27 13:02:29.043726 - epoch [12/30] loss: 0.0091 val acc: 97.70%\n",
      "2023-09-27 13:04:45.627926 - epoch [13/30] loss: 0.0258 val acc: 97.41%\n",
      "2023-09-27 13:07:04.328352 - epoch [14/30] loss: 0.0206 val acc: 97.60%\n",
      "2023-09-27 13:09:19.122710 - epoch [15/30] loss: 0.0060 val acc: 97.69%\n",
      "2023-09-27 13:11:33.815114 - epoch [16/30] loss: 0.0114 val acc: 97.64%\n",
      "2023-09-27 13:13:48.901558 - epoch [17/30] loss: 0.0130 val acc: 97.67%\n",
      "early stop\n",
      "end: 2023-09-27 13:13:48.902296\n",
      "elapsed: 0:00:00\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "print(f'start: {start_time}')\n",
    "\n",
    "model = CharCNN(num_features, embed_dim, num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), '../model/text_classification/brand_category_classification/char-cnn2_emb_dim_128.model')\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    current_time = datetime.datetime.now()\n",
    "    print(f'{current_time} - epoch [{epoch + 1}/{num_epochs}] loss: {loss.item():.4f} val acc: {accuracy:.2f}%')\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(\"early stop\")\n",
    "        break\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print(f'end: {end_time}')\n",
    "\n",
    "elapsed_time = start_time - start_time\n",
    "print(f'elapsed: {elapsed_time}')\n",
    "\n",
    "model.load_state_dict(torch.load('../model/text_classification/brand_category_classification/char-cnn2_emb_dim_128.model'))\n",
    "# torch.save(model.state_dict(), '../model/text_classification/brand_category_classification/char-cnn2.model')\n",
    "\n",
    "print(f'model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_predictions.extend(predicted.tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-27 13:13:48.932611 - 정확도: 97.56, 정밀도: 99.08, 재현율: 95.98, F1 점수: 97.51\n"
     ]
    }
   ],
   "source": [
    "current_time = datetime.datetime.now()\n",
    "\n",
    "accuracy, precision, recall, f1 = evaluate_model(model, val_loader)\n",
    "print(f'{current_time} - 정확도: {accuracy * 100:.2f}, 정밀도: {precision * 100:.2f}, 재현율: {recall * 100:.2f}, F1 점수: {f1 * 100:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-27 13:14:00.357044 - 이십삼점오: brand\n",
      "2023-09-27 13:14:00.357044 - 니트: category\n",
      "2023-09-27 13:14:00.357044 - 바람막이: category\n",
      "2023-09-27 13:14:00.357044 - 후드티: category\n",
      "2023-09-27 13:14:00.357044 - 블랭크룸: brand\n",
      "2023-09-27 13:14:00.357044 - 가디건: category\n",
      "2023-09-27 13:14:00.357044 - 키링: category\n",
      "2023-09-27 13:14:00.357044 - 모자: category\n",
      "2023-09-27 13:14:00.357044 - 후드집업: category\n",
      "2023-09-27 13:14:00.357044 - 오패: brand\n"
     ]
    }
   ],
   "source": [
    "def predict_text(text, model, char_to_idx, max_seq_length):\n",
    "    char_indices = [char_to_idx[c] for c in text if c in char_to_idx]\n",
    "    char_indices += [0] * (max_seq_length - len(char_indices))\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    inputs = torch.tensor(char_indices).unsqueeze(0)\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    return predicted.item()\n",
    "\n",
    "current_time = datetime.datetime.now()\n",
    "\n",
    "texts = [\"이십삼점오\", \"니트\", \"바람막이\", \"후드티\", \"블랭크룸\", \"가디건\", \"키링\", \"모자\", \"후드집업\", \"오패\"]\n",
    "\n",
    "for text in texts:\n",
    "    predicted_class = predict_text(text, model, char_to_idx, max_length)\n",
    "    print(f\"{current_time} - {text}: {'category' if predicted_class == 1 else 'brand'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created predicts.csv, 2023-09-27 13:24:14.348224\n"
     ]
    }
   ],
   "source": [
    "top_query_df = pd.read_csv('../data/text_classification/brand_category_classification/top_10000_query.csv')\n",
    "\n",
    "test_keywords = top_query_df['검색어'].to_list()\n",
    "result = {'keyword': [], 'predicted_label': []}\n",
    "all_predictions = []\n",
    "\n",
    "for text in test_keywords:\n",
    "    predicted_class = predict_text(text, model, char_to_idx, max_length)\n",
    "    all_predictions.append(predicted_class)\n",
    "\n",
    "    result['keyword'].append(text)\n",
    "    result['predicted_label'].append('category' if predicted_class == 1 else 'brand')\n",
    "\n",
    "current_time = datetime.datetime.now()\n",
    "    \n",
    "predict_df = pd.DataFrame.from_dict(result)\n",
    "predict_df.to_csv('../model/text_classification/brand_category_classification/char-cnn2_emb_dim_128_predicts.csv', index=False)\n",
    "\n",
    "print(f\"created predicts.csv, {current_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10,000 query에 대한 정확도: 0.93, 정밀도: 0.93, 재현율: 0.78\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(top_query_df['Type'].to_list())\n",
    "\n",
    "test_labels = label_encoder.fit_transform(top_query_df['Type'].to_list())\n",
    "\n",
    "accuracy = accuracy_score(test_labels, all_predictions)\n",
    "precision = precision_score(test_labels, all_predictions)\n",
    "recall = recall_score(test_labels, all_predictions)\n",
    "f1 = f1_score(test_labels, all_predictions)\n",
    "\n",
    "print(f'Top 10,000 query에 대한 정확도: {accuracy:.2f}, 정밀도: {precision:.2f}, 재현율: {recall:.2f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
