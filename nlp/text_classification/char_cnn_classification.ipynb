{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_types_df = pd.read_csv('../data/text_classification/keyword_types.csv')\n",
    "similar_keyword_df = pd.read_csv('../data/text_classification/similar_keyword.csv')\n",
    "category_df = pd.read_csv('../data/text_classification/category.csv')\n",
    "app_search_keyword_df = pd.read_csv('../data/text_classification/09_03_app_search_keyword.csv')\n",
    "brand_df = pd.read_csv('../data/text_classification/brands.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_keywords = [keyword for keywords in similar_keyword_df['similar_keyword'].to_list() for keyword in keywords.split(',') if keyword != '']\n",
    "category_labels = ['category' for _ in category_keywords]\n",
    "\n",
    "similar_keyword_category_df = pd.DataFrame({'keyword': category_keywords, 'label': category_labels})\n",
    "\n",
    "app_search_keyword_df = app_search_keyword_df[['keyword', 'brand_name']]\n",
    "app_search_keyword_df = app_search_keyword_df[~app_search_keyword_df.apply(lambda row: row['brand_name'] in row['keyword'], axis=1)]\n",
    "app_search_keyword_df = app_search_keyword_df[~app_search_keyword_df.apply(lambda row: row['keyword'] in row['brand_name'], axis=1)]\n",
    "app_search_keyword_df = app_search_keyword_df[~app_search_keyword_df.apply(lambda row: row['keyword'] in row['brand_name'], axis=1)]\n",
    "app_search_keyword_df = app_search_keyword_df[~app_search_keyword_df['keyword'].astype(str).str.isdigit()]\n",
    "app_search_keyword_df = app_search_keyword_df[~app_search_keyword_df['keyword'].str.contains(r'^[0-9!@#$%^&*()+]+$')]\n",
    "app_search_keyword_df = app_search_keyword_df[~app_search_keyword_df['keyword'].astype(str).str.match(r'^\\d+\\.\\d+$')]\n",
    "app_search_keyword_df = app_search_keyword_df[~(app_search_keyword_df['keyword'].str.replace(r'\\s+', '', regex=True) == app_search_keyword_df['brand_name'].str.replace(r'\\s+', '', regex=True))]\n",
    "app_search_keyword_df = app_search_keyword_df[~app_search_keyword_df.apply(lambda row: row['brand_name'].replace(\" \", \"\") in row['keyword'], axis=1)]\n",
    "app_search_keyword_df = app_search_keyword_df[~app_search_keyword_df.apply(lambda row: row['brand_name'].replace(\" \", \"\") in row['keyword'].replace(\" \", \"\"), axis=1)]\n",
    "app_search_keyword_df = app_search_keyword_df[~app_search_keyword_df['brand_name'].isin(['게스언더웨어', '24/7 시리즈', '24/7 시리즈 포 우먼', '호텔파리칠', '홈그로운 서플라이', '호와스', '헤지스골프', '헤이', '하킷', '헤라', '헤레우'])]\n",
    "app_search_keyword_df = app_search_keyword_df[~app_search_keyword_df['keyword'].isin(['호텔파리칠'])]\n",
    "app_search_keyword_df['keyword'] = app_search_keyword_df['keyword'].str.replace(r'\\s+', '', regex=True)\n",
    "app_search_keyword_df = app_search_keyword_df.drop_duplicates(subset='keyword', keep='first')\n",
    "\n",
    "app_search_keywords = app_search_keyword_df['keyword'].to_list()\n",
    "app_search_keywords_labels = ['category' for _ in app_search_keywords]\n",
    "\n",
    "app_search_category_df = pd.DataFrame({'keyword': app_search_keywords, 'label': app_search_keywords_labels})\n",
    "\n",
    "brands = brand_df['front_brand_name_kor'].to_list()[:4130]\n",
    "brand_labels = ['brand' for _ in brands]\n",
    "\n",
    "brand_keyword_df = pd.DataFrame({'keyword': brands, 'label': brand_labels})\n",
    "\n",
    "keyword_df = pd.concat([keyword_types_df, similar_keyword_category_df, category_df, app_search_category_df, brand_keyword_df])\n",
    "\n",
    "# keyword_df.drop_duplicates(inplace=True)\n",
    "keyword_df['keyword'] = keyword_df['keyword'].astype('str')\n",
    "keyword_df = keyword_df.sample(frac = 1)\n",
    "\n",
    "print(len(brands))\n",
    "print(len(keyword_df[keyword_df['label'] == 'brand']))\n",
    "print(len(keyword_df[keyword_df['label'] == 'category']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "brand_df = keyword_df[keyword_df['label'] == 'brand'][:30000]\n",
    "category_df = keyword_df[keyword_df['label'] == 'category'][:30000]\n",
    "\n",
    "keyword_df = pd.concat([brand_df, category_df])\n",
    "keyword_df = keyword_df.sample(frac = 1)\n",
    "\n",
    "print(len(keyword_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, char_to_idx, max_seq_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        char_indices = [self.char_to_idx[c] for c in text if c in self.char_to_idx]\n",
    "        char_indices += [0] * (self.max_seq_length - len(char_indices))\n",
    "\n",
    "        return torch.tensor(char_indices), torch.tensor(label)\n",
    "\n",
    "class CharCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super(CharCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # 논문에서는 256차원의 임베딩을 사용\n",
    "        self.conv1 = nn.Conv1d(512, 512, kernel_size=7, padding=0)  # 커널 크기 7\n",
    "        self.conv2 = nn.Conv1d(512, 512, kernel_size=7, padding=0)\n",
    "        self.conv3 = nn.Conv1d(512, 512, kernel_size=3, padding=0)  # 커널 크기 3\n",
    "        self.conv4 = nn.Conv1d(512, 512, kernel_size=3, padding=0)\n",
    "        self.fc1 = nn.Linear(1024, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x1 = F.relu(self.conv1(x))\n",
    "        x2 = F.relu(self.conv2(x))\n",
    "        x3 = F.relu(self.conv3(x))\n",
    "        x4 = F.relu(self.conv4(x))\n",
    "        x1 = F.max_pool1d(x1, x1.size(2)).squeeze(2)\n",
    "        x2 = F.max_pool1d(x2, x2.size(2)).squeeze(2)\n",
    "        x3 = F.max_pool1d(x3, x3.size(2)).squeeze(2)\n",
    "        x4 = F.max_pool1d(x4, x4.size(2)).squeeze(2)\n",
    "        x = torch.cat((x1, x2, x3, x4), 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 128\n",
    "embed_dim = 512\n",
    "num_classes = 2\n",
    "max_seq_length = 100\n",
    "learning_rate = 0.01\n",
    "batch_size = 4096\n",
    "num_epochs = 100\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(keyword_df['label'].to_list())\n",
    "\n",
    "texts = [keyword.replace(\" \", \"\") for keyword in keyword_df['keyword'].to_list()]\n",
    "labels = label_encoder.transform(keyword_df['label'].to_list())\n",
    "\n",
    "char_to_idx = {chr(i): i for i in range(128)}  # 문자를 인덱스로 매핑\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "train_dataset = CustomDataset(train_texts, train_labels, char_to_idx, max_seq_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(val_texts, val_labels, char_to_idx, max_seq_length)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model = CharCNN(vocab_size, embed_dim, num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # if val_loss < best_val_loss:\n",
    "    #     best_val_loss = val_loss\n",
    "    #     torch.save(model.state_dict(), 'best_model.pth')\n",
    "    #     counter = 0\n",
    "    # else:\n",
    "    #     counter += 1\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'epoch [{epoch + 1}/{num_epochs}] loss: {loss.item():.4f} val acc: {accuracy:.2f}%')\n",
    "\n",
    "    # if counter >= patience:\n",
    "    #     print(\"조기 종료: 검증 손실이 더 이상 감소하지 않습니다.\")\n",
    "    #     break\n",
    "\n",
    "# model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_predictions.extend(predicted.tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "accuracy, precision, recall, f1 = evaluate_model(model, val_loader)\n",
    "print(f'정확도: {accuracy:.2f}, 정밀도: {precision:.2f}, 재현율: {recall:.2f}, F1 점수: {f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(text, model, char_to_idx, max_seq_length):\n",
    "    char_indices = [char_to_idx[c] for c in text if c in char_to_idx]\n",
    "    char_indices += [0] * (max_seq_length - len(char_indices))\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    inputs = torch.tensor(char_indices).unsqueeze(0)\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    return predicted.item()\n",
    "\n",
    "\n",
    "texts = [\"이십삼점오\", \"니트\", \"바람막이\", \"후드티\", \"블랭크룸\", \"가디건\", \"키링\", \"모자\", \"후드집업\", \"오패\"]\n",
    "\n",
    "for text in texts:\n",
    "    predicted_class = predict_text(text, model, char_to_idx, max_seq_length)\n",
    "    print(f\"{text}: {'category' if predicted_class == 1 else 'brand'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
